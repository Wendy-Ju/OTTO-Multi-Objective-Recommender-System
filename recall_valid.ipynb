{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005753,
     "end_time": "2022-11-16T20:12:42.198693",
     "exception": false,
     "start_time": "2022-11-16T20:12:42.192940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Validation Notebook for My Candidate ReRank Model\n",
    "In this notebook we compute validation score for my other notebook [here][10] which submits an LB 0.575 solution. To compute validation, we just need to load parquet files from a different Kaggle dataset. Instead of loading the real train and real test data. We load the first 3 week of original train as \"new train\". And the last 1 week of original train as \"new test\". Then we train our model with \"new train\" and predict \"new test\". Finally we compute competition metric from our predictions. The data and code for validation comes from Radek [here][11].\n",
    "\n",
    "# Notes\n",
    "Below are notes about versions:\n",
    "* **Version 2 CV 0.5630** is validation for Candidate Rerank notebook version 1 with LB `0.573` \n",
    "* **Version 3 CV 0.5633** is validation for Candidate Rerank notebook version 2 with LB `0.573+`\n",
    "* **Version 4** is the same as version 3 but 1.5x faster co-visition matrix computation!\n",
    "* **Version 5 CV 0.5647** is validation for Candidate Rerank notebook version 4 with LB `0.575`\n",
    "* **Version 6** is the same as version 5 but 2x faster co-visitation matrix computation! (and 3x faster than version 2)\n",
    "* **Version 7** Stay tuned for more versions...\n",
    "\n",
    "# Introduction from My Candidate ReRank Notebook\n",
    "In this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n",
    "\n",
    "Note in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n",
    "\n",
    "### Step 1 - Generate Candidates\n",
    "For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n",
    "* User history of clicks, carts, orders\n",
    "* Most popular 20 clicks, carts, orders during test week\n",
    "* Co-visitation matrix of click/cart/order to cart/order with type weighting\n",
    "* Co-visitation matrix of cart/order to cart/order called buy2buy\n",
    "* Co-visitation matrix of click/cart/order to clicks with time weighting\n",
    "\n",
    "### Step 2 - ReRank and Choose 20\n",
    "Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n",
    "* Most recent previously visited items\n",
    "* Items previously visited multiple times\n",
    "* Items previously in cart or order\n",
    "* Co-visitation matrix of cart/order to cart/order\n",
    "* Current popular items\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n",
    "  \n",
    "# Credits\n",
    "We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n",
    "[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n",
    "[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n",
    "[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n",
    "[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n",
    "[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n",
    "[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n",
    "[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n",
    "[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n",
    "[10]: https://www.kaggle.com/cdeotte/candidate-rerank-model-lb-0-574\n",
    "[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004341,
     "end_time": "2022-11-16T20:12:42.207882",
     "exception": false,
     "start_time": "2022-11-16T20:12:42.203541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Candidate Generation with RAPIDS\n",
    "For candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:12:42.219675Z",
     "iopub.status.busy": "2022-11-16T20:12:42.218861Z",
     "iopub.status.idle": "2022-11-16T20:12:44.518641Z",
     "shell.execute_reply": "2022-11-16T20:12:44.516189Z"
    },
    "papermill": {
     "duration": 2.308288,
     "end_time": "2022-11-16T20:12:44.520806",
     "exception": false,
     "start_time": "2022-11-16T20:12:42.212518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 6\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "recallnum = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004736,
     "end_time": "2022-11-16T20:12:44.530523",
     "exception": false,
     "start_time": "2022-11-16T20:12:44.525787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Three Co-visitation Matrices with RAPIDS\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-11-16T20:12:44.541605Z",
     "iopub.status.busy": "2022-11-16T20:12:44.540916Z",
     "iopub.status.idle": "2022-11-16T20:13:26.763142Z",
     "shell.execute_reply": "2022-11-16T20:13:26.762144Z"
    },
    "papermill": {
     "duration": 42.235614,
     "end_time": "2022-11-16T20:13:26.770806",
     "exception": false,
     "start_time": "2022-11-16T20:12:44.535192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 120 files, in groups of 5 and chunks of 20.\n",
      "CPU times: user 43.4 s, sys: 8.1 s, total: 51.5 s\n",
      "Wall time: 35.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS 读数据\n",
    "def read_file(f):\n",
    "    return pd.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('../input/otto-validation/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2022-11-16T20:13:26.780357",
     "exception": false,
     "start_time": "2022-11-16T20:13:26.775684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:13:26.791160Z",
     "iopub.status.busy": "2022-11-16T20:13:26.790860Z",
     "iopub.status.idle": "2022-11-16T20:15:38.687369Z",
     "shell.execute_reply": "2022-11-16T20:15:38.686158Z"
    },
    "papermill": {
     "duration": 131.904621,
     "end_time": "2022-11-16T20:15:38.689682",
     "exception": false,
     "start_time": "2022-11-16T20:13:26.785061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n"
     ]
    }
   ],
   "source": [
    "type_weight = {0:1, 1:6, 2:3}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = pd.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)  ##只选30个\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]  #限制访问的时间间隔\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)   \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0103,
     "end_time": "2022-11-16T20:15:38.710395",
     "exception": false,
     "start_time": "2022-11-16T20:15:38.700095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-11-16T20:15:38.732525Z",
     "iopub.status.busy": "2022-11-16T20:15:38.732223Z",
     "iopub.status.idle": "2022-11-16T20:15:57.829417Z",
     "shell.execute_reply": "2022-11-16T20:15:57.828000Z"
    },
    "papermill": {
     "duration": 19.110941,
     "end_time": "2022-11-16T20:15:57.831598",
     "exception": false,
     "start_time": "2022-11-16T20:15:38.720657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n"
     ]
    }
   ],
   "source": [
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = pd.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1) \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS  #限制访问的时间间隔\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<40].drop('n',axis=1)###save more   change 15 to 40\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011523,
     "end_time": "2022-11-16T20:15:57.855624",
     "exception": false,
     "start_time": "2022-11-16T20:15:57.844101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-11-16T20:15:57.880467Z",
     "iopub.status.busy": "2022-11-16T20:15:57.880142Z",
     "iopub.status.idle": "2022-11-16T20:18:06.938665Z",
     "shell.execute_reply": "2022-11-16T20:18:06.937418Z"
    },
    "papermill": {
     "duration": 129.089436,
     "end_time": "2022-11-16T20:18:06.956608",
     "exception": false,
     "start_time": "2022-11-16T20:15:57.867172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "CPU times: user 1h 9min 55s, sys: 24min 44s, total: 1h 34min 40s\n",
      "Wall time: 1h 32min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = pd.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ] #限制访问的时间间隔\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<50].drop('n',axis=1)###save more   change 20 to 50\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:18:06.993817Z",
     "iopub.status.busy": "2022-11-16T20:18:06.993515Z",
     "iopub.status.idle": "2022-11-16T20:18:07.138162Z",
     "shell.execute_reply": "2022-11-16T20:18:07.137153Z"
    },
    "papermill": {
     "duration": 0.16592,
     "end_time": "2022-11-16T20:18:07.140416",
     "exception": false,
     "start_time": "2022-11-16T20:18:06.974496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # FREE MEMORY\n",
    "# del data_cache, tmp\n",
    "# _ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017902,
     "end_time": "2022-11-16T20:18:07.176982",
     "exception": false,
     "start_time": "2022-11-16T20:18:07.159080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
    "For description of the handcrafted rules, read this notebook's intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:18:07.215271Z",
     "iopub.status.busy": "2022-11-16T20:18:07.214314Z",
     "iopub.status.idle": "2022-11-16T20:18:09.106102Z",
     "shell.execute_reply": "2022-11-16T20:18:09.105064Z"
    },
    "papermill": {
     "duration": 1.913653,
     "end_time": "2022-11-16T20:18:09.108575",
     "exception": false,
     "start_time": "2022-11-16T20:18:07.194922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (7683577, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12719662</td>\n",
       "      <td>1628069</td>\n",
       "      <td>1661685778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12719663</td>\n",
       "      <td>523234</td>\n",
       "      <td>1661685778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12719663</td>\n",
       "      <td>1316756</td>\n",
       "      <td>1661686529</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12719663</td>\n",
       "      <td>424606</td>\n",
       "      <td>1661686795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12719663</td>\n",
       "      <td>1316756</td>\n",
       "      <td>1661686808</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session      aid          ts  type\n",
       "0  12719662  1628069  1661685778     0\n",
       "1  12719663   523234  1661685778     0\n",
       "2  12719663  1316756  1661686529     0\n",
       "3  12719663   424606  1661686795     0\n",
       "4  12719663  1316756  1661686808     0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('../input/otto-validation/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:18:09.145999Z",
     "iopub.status.busy": "2022-11-16T20:18:09.145032Z",
     "iopub.status.idle": "2022-11-16T20:19:42.638892Z",
     "shell.execute_reply": "2022-11-16T20:19:42.637891Z"
    },
    "papermill": {
     "duration": 93.53216,
     "end_time": "2022-11-16T20:19:42.658626",
     "exception": false,
     "start_time": "2022-11-16T20:18:09.126466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are size of our 3 co-visitation matrices:\n",
      "1812132 1055146 1812132\n",
      "CPU times: user 2min 40s, sys: 18.9 s, total: 2min 59s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
    "top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
    "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:recallnum]\n",
    "top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:recallnum]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:19:42.695721Z",
     "iopub.status.busy": "2022-11-16T20:19:42.695387Z",
     "iopub.status.idle": "2022-11-16T20:19:42.737339Z",
     "shell.execute_reply": "2022-11-16T20:19:42.736659Z"
    },
    "papermill": {
     "duration": 0.063054,
     "end_time": "2022-11-16T20:19:42.739199",
     "exception": false,
     "start_time": "2022-11-16T20:19:42.676145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=50:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(recallnum)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(recallnum) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:recallnum - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:recallnum-len(result)]\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=50:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(recallnum)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(recallnum) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:recallnum - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:recallnum-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017308,
     "end_time": "2022-11-16T20:19:42.774236",
     "exception": false,
     "start_time": "2022-11-16T20:19:42.756928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV\n",
    "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-16T20:19:42.814456Z",
     "iopub.status.busy": "2022-11-16T20:19:42.814017Z",
     "iopub.status.idle": "2022-11-16T20:47:19.141226Z",
     "shell.execute_reply": "2022-11-16T20:47:19.140196Z"
    },
    "papermill": {
     "duration": 1656.369652,
     "end_time": "2022-11-16T20:47:19.161343",
     "exception": false,
     "start_time": "2022-11-16T20:19:42.791691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 51s, sys: 28.2 s, total: 37min 20s\n",
      "Wall time: 37min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:47:19.200068Z",
     "iopub.status.busy": "2022-11-16T20:47:19.198342Z",
     "iopub.status.idle": "2022-11-16T20:47:22.311750Z",
     "shell.execute_reply": "2022-11-16T20:47:22.310731Z"
    },
    "papermill": {
     "duration": 3.135191,
     "end_time": "2022-11-16T20:47:22.314251",
     "exception": false,
     "start_time": "2022-11-16T20:47:19.179060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:47:22.351637Z",
     "iopub.status.busy": "2022-11-16T20:47:22.351333Z",
     "iopub.status.idle": "2022-11-16T20:48:04.306077Z",
     "shell.execute_reply": "2022-11-16T20:48:04.305142Z"
    },
    "papermill": {
     "duration": 41.997207,
     "end_time": "2022-11-16T20:48:04.329742",
     "exception": false,
     "start_time": "2022-11-16T20:47:22.332535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 612920 1673641 1202618 1159...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...\n",
       "1  11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...\n",
       "2  11098530_clicks  409236 264500 1603001 963957 254154 583026 167...\n",
       "3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n",
       "4  11098532_clicks  876469 7651 108125 612920 1673641 1202618 1159..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(\"validation_preds.csv\", index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017837,
     "end_time": "2022-11-16T20:48:04.366380",
     "exception": false,
     "start_time": "2022-11-16T20:48:04.348543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compute Validation Score\n",
    "This code is from Radek [here][1]. It has been modified to use less memory.\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T20:48:07.244310Z",
     "iopub.status.busy": "2022-11-16T20:48:07.243961Z",
     "iopub.status.idle": "2022-11-16T20:49:55.558821Z",
     "shell.execute_reply": "2022-11-16T20:49:55.554648Z"
    },
    "papermill": {
     "duration": 108.351524,
     "end_time": "2022-11-16T20:49:55.576479",
     "exception": false,
     "start_time": "2022-11-16T20:48:07.224955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5232954759064763\n",
      "carts recall = 0.4065799105609542\n",
      "orders recall = 0.6332751362099948\n",
      "=============\n",
      "Overall Recall = 0.5542686024849308\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE METRIC\n",
    "score = 0\n",
    "weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "for t in ['clicks','carts','orders']:\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "    test_labels = pd.read_parquet('../input/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==t]\n",
    "    test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    score += weights[t]*recall\n",
    "    print(f'{t} recall =',recall)\n",
    "    \n",
    "print('=============')\n",
    "print('Overall Recall =',score)\n",
    "print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pred_df = pd.read_csv(\"validation_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['session'] = pred_df.session_type.apply(lambda x: int(x.split('_')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n",
       "      <td>11098528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n",
       "      <td>11098529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n",
       "      <td>11098530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "      <td>11098531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 612920 1673641 1202618 1159...</td>\n",
       "      <td>11098532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403748</th>\n",
       "      <td>12899774_carts</td>\n",
       "      <td>33035 1539309 819288 95488 270852 771913 74397...</td>\n",
       "      <td>12899774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403749</th>\n",
       "      <td>12899775_carts</td>\n",
       "      <td>1743151 1760714 1163166 1255910 1498443 832192...</td>\n",
       "      <td>12899775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403750</th>\n",
       "      <td>12899776_carts</td>\n",
       "      <td>548599 1401030 1150130 1440959 1144446 1330306...</td>\n",
       "      <td>12899776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403751</th>\n",
       "      <td>12899777_carts</td>\n",
       "      <td>384045 1308634 395762 1688215 1486067 703474 2...</td>\n",
       "      <td>12899777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403752</th>\n",
       "      <td>12899778_carts</td>\n",
       "      <td>561560 1167224 13942 566042 570506 828726 9715...</td>\n",
       "      <td>12899778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5403753 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels  \\\n",
       "0        11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...   \n",
       "1        11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...   \n",
       "2        11098530_clicks  409236 264500 1603001 963957 254154 583026 167...   \n",
       "3        11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...   \n",
       "4        11098532_clicks  876469 7651 108125 612920 1673641 1202618 1159...   \n",
       "...                  ...                                                ...   \n",
       "5403748   12899774_carts  33035 1539309 819288 95488 270852 771913 74397...   \n",
       "5403749   12899775_carts  1743151 1760714 1163166 1255910 1498443 832192...   \n",
       "5403750   12899776_carts  548599 1401030 1150130 1440959 1144446 1330306...   \n",
       "5403751   12899777_carts  384045 1308634 395762 1688215 1486067 703474 2...   \n",
       "5403752   12899778_carts  561560 1167224 13942 566042 570506 828726 9715...   \n",
       "\n",
       "          session  \n",
       "0        11098528  \n",
       "1        11098529  \n",
       "2        11098530  \n",
       "3        11098531  \n",
       "4        11098532  \n",
       "...           ...  \n",
       "5403748  12899774  \n",
       "5403749  12899775  \n",
       "5403750  12899776  \n",
       "5403751  12899777  \n",
       "5403752  12899778  \n",
       "\n",
       "[5403753 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2243.999209,
   "end_time": "2022-11-16T20:49:58.518056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-16T20:12:34.518847",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
